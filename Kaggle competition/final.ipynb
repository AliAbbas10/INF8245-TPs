{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3c1916",
   "metadata": {},
   "source": [
    "**Structure:**\n",
    "1. **Pre-processing** : Data loading and exploration\n",
    "2. **Feature Selection** : Dimensionality reduction\n",
    "3. **Validation** : Cross-validation setup\n",
    "4. **Algorithms and Optimization** : Model training and comparison\n",
    "5. **Submisisons** : Model selection and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752598a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Pre-processing\n",
    "\n",
    "Loading and analyzing the dataset\n",
    "\n",
    "### 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f97806",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('data/train.npz')\n",
    "test = np.load('data/test.npz')\n",
    "\n",
    "X_train = train['X_train']\n",
    "y_train = train['y_train']\n",
    "train_ids = train['ids']\n",
    "\n",
    "X_test = test['X_test']\n",
    "test_ids = test['ids']\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]:,} samples x {X_train.shape[1]:,} features\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train.shape\n",
    "n_susceptible = np.sum(y_train == 0)\n",
    "n_resistant = np.sum(y_train == 1)\n",
    "imbalance_ratio = n_susceptible / n_resistant\n",
    "\n",
    "non_zero = np.count_nonzero(X_train)\n",
    "total_entries = n_samples * n_features\n",
    "sparsity = 100 * (1 - non_zero / total_entries)\n",
    "\n",
    "print(f\"Samples: {n_samples:,} , Features: {n_features:,}\")\n",
    "print(f\"Class distribution: {n_susceptible:,} susceptible ({100*n_susceptible/n_samples:.1f}%), {n_resistant:,} resistant ({100*n_resistant/n_samples:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Matrix sparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73049e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "class_labels = ['Susceptible', 'Resistant']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "bars = plt.bar(class_labels, class_counts.values, color=['#2ecc71', '#e74c3c'], alpha=0.5)\n",
    "plt.ylabel('Count', fontsize=11)\n",
    "plt.title('Class Distribution', fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, class_counts.values)):\n",
    "    percentage = 100 * count / len(y_train)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n",
    "             f'{count:,}\\n({percentage:.1f}%)', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68088f",
   "metadata": {},
   "source": [
    "### 1.2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of features with Chi-square\n",
    "k_values = [5000, 10000, 15000, 20000, 30000]\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.01)\n",
    "X_train_var = variance_threshold.fit_transform(X_train)\n",
    "X_test_var = variance_threshold.transform(X_test)\n",
    "\n",
    "print(f\"After variance threshold: {X_train_var.shape[1]:,} features\\n\")\n",
    "\n",
    "results = []\n",
    "for k in k_values:\n",
    "    if k > X_train_var.shape[1]:\n",
    "        continue\n",
    "    \n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    X_train_k = selector.fit_transform(X_train_var, y_train)\n",
    "    X_test_k = selector.transform(X_test_var)\n",
    "    \n",
    "    # test with Logistic Regression\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(lr, X_train_k, y_train, cv=cv, scoring=make_scorer(f1_score, average='macro'), n_jobs=-1)\n",
    "    \n",
    "    results.append({'k': k, 'cv_f1': scores.mean(), 'cv_std': scores.std()})\n",
    "    print(f\"K={k:6,}: CV F1 = {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "\n",
    "best_result = max(results, key=lambda x: x['cv_f1'])\n",
    "K_FEATURES = best_result['k']\n",
    "print(f\"\\nBest K: {K_FEATURES:,} features (CV F1 = {best_result['cv_f1']:.4f})\")\n",
    "\n",
    "# Apply best K\n",
    "selector = SelectKBest(chi2, k=K_FEATURES)\n",
    "X_train_selected = selector.fit_transform(X_train_var, y_train)\n",
    "X_test_selected = selector.transform(X_test_var)\n",
    "\n",
    "print(f\"\\nFinal feature selection: {X_train_selected.shape[1]:,} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65eab6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Selection\n",
    "\n",
    "Applying filtering to reduce from 1,000,000 to 30,000 features\n",
    "\n",
    "### 2.1 Feature Selection Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = selector.scores_\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(feature_scores, bins=50, color='#3498db', alpha=0.5)\n",
    "plt.xlabel('Score', fontsize=11)\n",
    "plt.ylabel('Frequency', fontsize=11)\n",
    "plt.title('Feature Score Distribution', fontsize=11)\n",
    "plt.yscale('log')\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54de0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Validation\n",
    "\n",
    "Setup cross-validation for model evaluation\n",
    "\n",
    "### 3.1 Validation Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c00964",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Algorithms & Optimization\n",
    "\n",
    "Train and optimize classification models with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90a633",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_baseline = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr_scores = cross_val_score(lr_baseline, X_train_selected, y_train, cv=cv, scoring=f1_scorer, n_jobs=-1)\n",
    "\n",
    "print(f\"Logistic Regression Baseline: CV F1 = {lr_scores.mean():.4f}\")\n",
    "\n",
    "lr_baseline.fit(X_train_selected, y_train)\n",
    "baseline_score = lr_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31fd02",
   "metadata": {},
   "source": [
    "### 4.2 Grid Search Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'max_depth': [10, 15, 20, 25, None],\n",
    "    'min_samples_split': [5, 10, 15, 20],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "lr_params = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9],\n",
    "    'scale_pos_weight': [scale_pos_weight]\n",
    "}\n",
    "\n",
    "print(f\"Using 5-fold cross-validation\")\n",
    "print(f\"Class imbalance ratio: {scale_pos_weight:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ac883",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "print(f\"Best RF: F1 = {rf_grid.best_score_:.4f}, Params = {rf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a496379",
   "metadata": {},
   "source": [
    "### 4.4 Logistic Regression Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65aec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, n_jobs=-1),\n",
    "    lr_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "print(f\"Best LR: F1 = {lr_grid.best_score_:.4f}, Params = {lr_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786720d",
   "metadata": {},
   "source": [
    "### 4.5 SVM Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f734e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(class_weight='balanced', random_state=42, cache_size=1000),\n",
    "    svm_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best SVM: F1 = {svm_grid.best_score_:.4f}, Params = {svm_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf5ce3",
   "metadata": {},
   "source": [
    "### 4.6 XGBoost Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0257969",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = GridSearchCV(\n",
    "    xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist'\n",
    "    ),\n",
    "    xgb_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "print(f\"Best XGBoost: F1 = {xgb_grid.best_score_:.4f}, Params = {xgb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9582d",
   "metadata": {},
   "source": [
    "### 4.7 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = pd.DataFrame(rf_grid.cv_results_)\n",
    "lr_results = pd.DataFrame(lr_grid.cv_results_)\n",
    "svm_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "xgb_results = pd.DataFrame(xgb_grid.cv_results_)\n",
    "\n",
    "rf_all = rf_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "lr_all = lr_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "svm_all = svm_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "xgb_all = xgb_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in rf_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'RandomForest',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "for idx, row in lr_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'LogisticRegression',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "for idx, row in svm_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'SVM',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "for idx, row in xgb_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results).sort_values('CV_F1', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Configurations:\")\n",
    "print(results_df.head(15).to_string(index=False))\n",
    "\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\nBest Model: {best['Model']} - CV F1 = {best['CV_F1']:.4f}\")\n",
    "\n",
    "results_df.to_csv('grid_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7359fa",
   "metadata": {},
   "source": [
    "## 5. Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef41c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.1 Random Forest Submission\n",
    "\n",
    "Generate submission using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7190b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    **rf_grid.best_params_,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_scores = cross_val_score(rf, X_train_selected, y_train, cv=cv, scoring=f1_scorer, n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train_selected, y_train)\n",
    "y_test_pred_rf = rf.predict(X_test_selected)\n",
    "\n",
    "submission_rf = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_rf\n",
    "})\n",
    "\n",
    "submission_rf.to_csv('rf_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_rf == 1)} ({100*np.sum(y_test_pred_rf == 1)/len(y_test_pred_rf):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfbe9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.2 SVM Submission\n",
    "\n",
    "Generate submission using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best = SVC(\n",
    "    **svm_grid.best_params_,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    cache_size=1000\n",
    ")\n",
    "\n",
    "svm_best_scores = cross_val_score(svm_best, X_train_scaled, y_train, cv=cv, scoring=f1_scorer, n_jobs=-1)\n",
    "\n",
    "svm_best.fit(X_train_scaled, y_train)\n",
    "y_test_pred_svm = svm_best.predict(X_test_scaled)\n",
    "\n",
    "submission_svm_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_svm\n",
    "})\n",
    "\n",
    "submission_svm_df.to_csv('svm_rbf_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_svm == 1)} ({100*np.sum(y_test_pred_svm == 1)/len(y_test_pred_svm):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33492c03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.3 XGBoost Submission\n",
    "\n",
    "Generate submission using best XGBoost configuration from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee34fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = xgb.XGBClassifier(\n",
    "    **xgb_grid.best_params_,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_best_scores = cross_val_score(xgb_best, X_train_selected, y_train, cv=cv, scoring=f1_scorer, n_jobs=-1)\n",
    "\n",
    "xgb_best.fit(X_train_selected, y_train)\n",
    "y_test_pred_xgb = xgb_best.predict(X_test_selected)\n",
    "\n",
    "submission_xgb_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_xgb\n",
    "})\n",
    "\n",
    "submission_xgb_df.to_csv('xgboost_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_xgb == 1)} ({100*np.sum(y_test_pred_xgb == 1)/len(y_test_pred_xgb):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfecc87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.4 Best Kaggle Submission\n",
    "\n",
    "Best performing XGBoost configuration (CV F1 = 0.8291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26847fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_kaggle = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=10,\n",
    "    n_estimators=300,\n",
    "    scale_pos_weight=6.102564102564102,\n",
    "    subsample=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_best_kaggle_scores = cross_val_score(xgb_best_kaggle, X_train_selected, y_train, cv=cv, scoring=f1_scorer, n_jobs=-1)\n",
    "\n",
    "\n",
    "xgb_best_kaggle.fit(X_train_selected, y_train)\n",
    "y_test_pred_best_kaggle = xgb_best_kaggle.predict(X_test_selected)\n",
    "\n",
    "submission_best_kaggle = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_best_kaggle\n",
    "})\n",
    "\n",
    "submission_best_kaggle.to_csv('best_kaggle_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_best_kaggle == 1)} ({100*np.sum(y_test_pred_best_kaggle == 1)/len(y_test_pred_best_kaggle):.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3c1916",
   "metadata": {},
   "source": [
    "**Sections:**\n",
    "1. **Pre-processing** : Data loading and exploration\n",
    "2. **Algorithms and Optimization** : Model training and comparison\n",
    "3. **Submissions** : Model selection and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752598a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Pre-processing\n",
    "\n",
    "Loading and analyzing the dataset\n",
    "\n",
    "### 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f97806",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('data/train.npz')\n",
    "test = np.load('data/test.npz')\n",
    "\n",
    "X_train = train['X_train']\n",
    "y_train = train['y_train']\n",
    "train_ids = train['ids']\n",
    "\n",
    "X_test = test['X_test']\n",
    "test_ids = test['ids']\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]:,} samples x {X_train.shape[1]:,} features\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a2d46",
   "metadata": {},
   "source": [
    "### 1.2 Data Exploration\n",
    "\n",
    "Analyzing class distribution and dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train.shape\n",
    "n_susceptible = np.sum(y_train == 0)\n",
    "n_resistant = np.sum(y_train == 1)\n",
    "imbalance_ratio = n_susceptible / n_resistant\n",
    "\n",
    "non_zero = np.count_nonzero(X_train)\n",
    "total_entries = n_samples * n_features\n",
    "sparsity = 100 * (1 - non_zero / total_entries)\n",
    "\n",
    "print(f\"Samples: {n_samples:,} , Features: {n_features:,}\")\n",
    "print(f\"Class distribution: {n_susceptible:,} susceptible ({100*n_susceptible/n_samples:.1f}%), {n_resistant:,} resistant ({100*n_resistant/n_samples:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Matrix sparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73049e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "class_labels = ['Susceptible', 'Resistant']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "bars = plt.bar(class_labels, class_counts.values, color=['#2ecc71', '#e74c3c'], alpha=0.5)\n",
    "plt.ylabel('Count', fontsize=11)\n",
    "plt.title('Class Distribution', fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, class_counts.values)):\n",
    "    percentage = 100 * count / len(y_train)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n",
    "             f'{count:,}\\n({percentage:.1f}%)', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68088f",
   "metadata": {},
   "source": [
    "### 1.3 Feature Selection\n",
    "\n",
    "Testing different feature counts and applying variance threshold and chi-square selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_values = [5000, 10000, 15000, 20000, 30000]\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.01)\n",
    "X_train_var = variance_threshold.fit_transform(X_train)\n",
    "X_test_var = variance_threshold.transform(X_test)\n",
    "\n",
    "print(f\"After variance threshold: {X_train_var.shape[1]:,} features\\n\")\n",
    "\n",
    "results = []\n",
    "for k in k_values:\n",
    "    if k > X_train_var.shape[1]:\n",
    "        continue\n",
    "    \n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    X_train_k = selector.fit_transform(X_train_var, y_train)\n",
    "    X_test_k = selector.transform(X_test_var)\n",
    "    \n",
    "    # test with Logistic Regression\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(lr, X_train_k, y_train, cv=cv, scoring=make_scorer(f1_score, average='macro'), n_jobs=-1)\n",
    "    \n",
    "    results.append({'k': k, 'cv_f1': scores.mean(), 'cv_std': scores.std()})\n",
    "    print(f\"K={k:6,}: CV F1 = {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "\n",
    "best_result = max(results, key=lambda x: x['cv_f1'])\n",
    "K_FEATURES = best_result['k']\n",
    "print(f\"\\nBest K: {K_FEATURES:,} features (CV F1 = {best_result['cv_f1']:.4f})\")\n",
    "\n",
    "# Apply best K\n",
    "selector = SelectKBest(chi2, k=K_FEATURES)\n",
    "X_train_selected = selector.fit_transform(X_train_var, y_train)\n",
    "X_test_selected = selector.transform(X_test_var)\n",
    "\n",
    "print(f\"\\nFinal feature selection: {X_train_selected.shape[1]:,} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c00964",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Training Algorithms\n",
    "\n",
    "Training classification models with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31fd02",
   "metadata": {},
   "source": [
    "### 2.1 Grid Search Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'max_depth': [10, 15, 20, 25, None],\n",
    "    'min_samples_split': [5, 10, 15, 20],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "lr_params = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9],\n",
    "    'scale_pos_weight': [scale_pos_weight]\n",
    "}\n",
    "\n",
    "print(f\"Using 5-fold cross-validation\")\n",
    "print(f\"Class imbalance ratio: {scale_pos_weight:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ac883",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "print(f\"Best RF: F1 = {rf_grid.best_score_:.4f}, Params = {rf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a496379",
   "metadata": {},
   "source": [
    "### 2.3 Logistic Regression Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65aec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, n_jobs=-1),\n",
    "    lr_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "print(f\"Best LR: F1 = {lr_grid.best_score_:.4f}, Params = {lr_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786720d",
   "metadata": {},
   "source": [
    "### 2.4 SVM Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f734e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(class_weight='balanced', random_state=42, cache_size=1000),\n",
    "    svm_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best SVM: F1 = {svm_grid.best_score_:.4f}, Params = {svm_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf5ce3",
   "metadata": {},
   "source": [
    "### 2.5 XGBoost Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0257969",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = GridSearchCV(\n",
    "    xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist'\n",
    "    ),\n",
    "    xgb_params,\n",
    "    cv=cv,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train_selected, y_train)\n",
    "\n",
    "print(f\"Best XGBoost: F1 = {xgb_grid.best_score_:.4f}, Params = {xgb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9582d",
   "metadata": {},
   "source": [
    "### 2.6 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = pd.DataFrame(rf_grid.cv_results_)\n",
    "lr_results = pd.DataFrame(lr_grid.cv_results_)\n",
    "svm_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "xgb_results = pd.DataFrame(xgb_grid.cv_results_)\n",
    "\n",
    "rf_all = rf_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "lr_all = lr_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "svm_all = svm_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "xgb_all = xgb_results.sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in rf_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'RandomForest',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "for idx, row in lr_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'LogisticRegression',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "for idx, row in svm_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'SVM',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "for idx, row in xgb_all.iterrows():\n",
    "    all_results.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Rank': int(row['rank_test_score']),\n",
    "        'CV_F1': row['mean_test_score'],\n",
    "        'CV_Std': row['std_test_score'],\n",
    "        'Config': str(row['params'])\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results).sort_values('CV_F1', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Configurations:\")\n",
    "print(results_df.head(15).to_string(index=False))\n",
    "\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\nBest Model: {best['Model']} CV F1 = {best['CV_F1']:.4f}\")\n",
    "\n",
    "results_df.to_csv('grid_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd164ee2",
   "metadata": {},
   "source": [
    "### 2.7 Results Visualization\n",
    "\n",
    "Visualizing model performance and hyperparameter impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406aac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_scores = {\n",
    "    'Random Forest': rf_grid.best_score_,\n",
    "    'Logistic Regression': lr_grid.best_score_,\n",
    "    'SVM': svm_grid.best_score_,\n",
    "    'XGBoost': xgb_grid.best_score_\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "\n",
    "models = list(best_scores.keys())\n",
    "scores = list(best_scores.values())\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "bars = ax.bar(models, scores, color=colors, alpha=0.5)\n",
    "ax.set_ylabel('CV F1 Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=12)\n",
    "ax.set_ylim(min(scores) - 0.01, max(scores) + 0.01)\n",
    "ax.grid(axis='y', alpha=0.5)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{score:.4f}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Model: {max(best_scores, key=best_scores.get)} (F1 = {max(best_scores.values()):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "rf_params_analysis = []\n",
    "for idx, row in rf_results.iterrows():\n",
    "    params = row['params']\n",
    "    rf_params_analysis.append({\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'score': row['mean_test_score']\n",
    "    })\n",
    "rf_df = pd.DataFrame(rf_params_analysis)\n",
    "rf_grouped = rf_df.groupby('n_estimators')['score'].mean().sort_index()\n",
    "\n",
    "axes[0].plot(rf_grouped.index, rf_grouped.values, marker='o', linewidth=2, markersize=8, color='blue', alpha=0.5)\n",
    "axes[0].set_xlabel('Number of Estimators', fontsize=12)\n",
    "axes[0].set_ylabel('Mean CV F1 Score', fontsize=12)\n",
    "axes[0].set_title('Random Forest: n_estimators Impact', fontsize=12)\n",
    "axes[0].grid(alpha=0.5)\n",
    "\n",
    "xgb_params_analysis = []\n",
    "for idx, row in xgb_results.iterrows():\n",
    "    params = row['params']\n",
    "    xgb_params_analysis.append({\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'score': row['mean_test_score']\n",
    "    })\n",
    "xgb_df = pd.DataFrame(xgb_params_analysis)\n",
    "xgb_grouped = xgb_df.groupby('learning_rate')['score'].mean().sort_index()\n",
    "\n",
    "axes[1].plot(xgb_grouped.index, xgb_grouped.values, marker='o', linewidth=2, markersize=8, color='orange', alpha=0.5)\n",
    "axes[1].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Mean CV F1 Score', fontsize=12)\n",
    "axes[1].set_title('XGBoost: Learning Rate Impact', fontsize=12)\n",
    "axes[1].grid(alpha=0.5)\n",
    "\n",
    "svm_params_analysis = []\n",
    "for idx, row in svm_results.iterrows():\n",
    "    params = row['params']\n",
    "    svm_params_analysis.append({\n",
    "        'kernel': params['kernel'],\n",
    "        'score': row['mean_test_score']\n",
    "    })\n",
    "svm_df = pd.DataFrame(svm_params_analysis)\n",
    "svm_grouped = svm_df.groupby('kernel')['score'].mean()\n",
    "\n",
    "axes[2].bar(svm_grouped.index, svm_grouped.values, color=['green', 'red'], alpha=0.5)\n",
    "axes[2].set_xlabel('Kernel Type', fontsize=12)\n",
    "axes[2].set_ylabel('Mean CV F1 Score', fontsize=12)\n",
    "axes[2].set_title('SVM: Kernel Comparison', fontsize=12)\n",
    "axes[2].grid(axis='y', alpha=0.5)\n",
    "\n",
    "for i, (kernel, score) in enumerate(svm_grouped.items()):\n",
    "    axes[2].text(i, score, f'{score:.4f}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hyperparameter_analysis.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca339e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_10 = results_df.head(10)[['Model', 'CV_F1', 'CV_Std', 'Config']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for idx, row in top_10.iterrows():\n",
    "    config_str = row['Config'].replace('{', '').replace('}', '').replace(\"'\", '')\n",
    "    if len(config_str) > 80:\n",
    "        config_str = config_str[:77] + '...'\n",
    "    table_data.append([\n",
    "        row['Model'],\n",
    "        f\"{row['CV_F1']:.4f}\",\n",
    "        f\"{row['CV_Std']:.4f}\",\n",
    "        config_str\n",
    "    ])\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Model', 'CV F1', 'CV Std', 'Configuration'],\n",
    "                cellLoc='left',\n",
    "                loc='center',\n",
    "                colWidths=[0.15, 0.08, 0.08, 0.69])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1, 2)\n",
    "\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('lightgray')\n",
    "\n",
    "colors = {'RandomForest': 'lightblue', 'XGBoost': 'lightyellow', 'SVM': 'lightgreen', 'LogisticRegression': 'lightpink'}\n",
    "for i, row in enumerate(top_10.iterrows(), 1):\n",
    "    model = row[1]['Model']\n",
    "    for j in range(4):\n",
    "        table[(i, j)].set_facecolor(colors.get(model, 'white'))\n",
    "\n",
    "plt.title('Top 10 Model Configurations', fontsize=12)\n",
    "plt.savefig('top_configurations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7359fa",
   "metadata": {},
   "source": [
    "## 3. Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef41c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.1 Random Forest Submission\n",
    "\n",
    "Generate submission using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7190b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    **rf_grid.best_params_,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train_selected, y_train)\n",
    "y_test_pred_rf = rf.predict(X_test_selected)\n",
    "\n",
    "submission_rf = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_rf\n",
    "})\n",
    "\n",
    "submission_rf.to_csv('rf_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_rf == 1)} ({100*np.sum(y_test_pred_rf == 1)/len(y_test_pred_rf):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35bafa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2 Logistic Regression Submission\n",
    "\n",
    "Generate submission using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00453f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    **lr_grid.best_params_,\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr.fit(X_train_selected, y_train)\n",
    "y_test_pred_lr = lr.predict(X_test_selected)\n",
    "\n",
    "submission_lr = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_lr\n",
    "})\n",
    "\n",
    "submission_lr.to_csv('lr_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_lr == 1)} ({100*np.sum(y_test_pred_lr == 1)/len(y_test_pred_lr):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfbe9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.3 SVM Submission\n",
    "\n",
    "Generate submission using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best = SVC(\n",
    "    **svm_grid.best_params_,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    cache_size=1000\n",
    ")\n",
    "\n",
    "svm_best.fit(X_train_scaled, y_train)\n",
    "y_test_pred_svm = svm_best.predict(X_test_scaled)\n",
    "\n",
    "submission_svm_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_svm\n",
    "})\n",
    "\n",
    "submission_svm_df.to_csv('svm_rbf_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_svm == 1)} ({100*np.sum(y_test_pred_svm == 1)/len(y_test_pred_svm):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33492c03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.4 XGBoost Submission\n",
    "\n",
    "Generate submission using best XGBoost configuration from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee34fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = xgb.XGBClassifier(\n",
    "    **xgb_grid.best_params_,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_best.fit(X_train_selected, y_train)\n",
    "y_test_pred_xgb = xgb_best.predict(X_test_selected)\n",
    "\n",
    "submission_xgb_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_xgb\n",
    "})\n",
    "\n",
    "submission_xgb_df.to_csv('xgboost_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_xgb == 1)} ({100*np.sum(y_test_pred_xgb == 1)/len(y_test_pred_xgb):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfecc87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.5 Best Kaggle Submission\n",
    "\n",
    "Best performing XGBoost configuration (CV F1 = 0.8291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26847fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_kaggle = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=10,\n",
    "    n_estimators=300,\n",
    "    scale_pos_weight=6.102564102564102,\n",
    "    subsample=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_best_kaggle.fit(X_train_selected, y_train)\n",
    "y_test_pred_best_kaggle = xgb_best_kaggle.predict(X_test_selected)\n",
    "\n",
    "submission_best_kaggle = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': y_test_pred_best_kaggle\n",
    "})\n",
    "\n",
    "submission_best_kaggle.to_csv('best_kaggle_submission.csv', index=False)\n",
    "print(f\"Predicted resistant: {np.sum(y_test_pred_best_kaggle == 1)} ({100*np.sum(y_test_pred_best_kaggle == 1)/len(y_test_pred_best_kaggle):.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
